\section{CONCLUSIONS and FUTURE WORK}

% We have shown that the computational efficiency of adversarial attacks on machine learning classifiers is improved with the implementation of PCA, which reduces the feature dimensionality by a factor of $8$.
%The method followed in this paper is two-fold. 
We attacked image classifiers using a two fold strategy: we first imitated the target classifier with a substitute LR model and then generated adversarial samples based on the substitute model \cite{papernot3}. To reduce the computational cost of crafting adversarial samples, we reduced the feature space dimensionality by utilizing PCA. Although PCA reduced the performance of the adversarial samples in FGS, it only had a small impact on the success of the Papernot algorithm, maintaining a reasonable misclassification rate of about $\sim 70\%$. Furthermore, although the runtime reduction due to PCA was negligible for FGS, it was significant for the Papernot algorithm, reducing the computation time by half. We have shown that we can increase the efficiency of adversarial sample construction while maintaining misclassification effectiveness utilizing the Papernot adversarial sample crafting method in combination with PCA feature reduction. 

%We found that PCA seemed to improve the quality of adversarial images for the FGS method, but compromised the quality of the adversarial images for the Papernot method. 

%The performance of the adversarial samples, although diminished, is still acceptably effective after introducing PCA to reduce the feature space. 

%Since the dimensionality reduction proved to be generally successful on the MNIST dataset, improving the efficiency of Papernot's algorithm in \cite{papernot3}, then 
For future work, the same approach could be applied to more complex images such as the GTSRB dataset of coloured traffic signs. The GTSRB images have a higher resolution, allowing for more subtle perturbations to be added to the samples. Furthermore, the algorithm presented in this paper should be extended to targeted misclassification. These additions would portray the inherent vulnerability of machine learning algorithms to adversarial attacks in such high-risk applications as autonomous car navigation (e.g. having the oracle read a stop sign as a yield sign \cite{papernot1}). 